# configs/lora.yaml
model:
  model_name: "bert-base-uncased"
  num_labels: 2

data:
  dataset_name: "imdb"
  max_length: 256

lora:
  r: 8
  lora_alpha: 32
  target_modules: ["query", "value"]
  lora_dropout: 0.05

training:
  output_dir: "./lora_results"
  learning_rate: 1e-4
  per_device_train_batch_size: 16
  num_train_epochs: 3
  fp16: true
  use_wandb: false
